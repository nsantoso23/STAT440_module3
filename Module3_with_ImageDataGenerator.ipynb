{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "# import PIL\n",
    "import matplotlib\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import BatchNormalization\n",
    "from keras import regularizers\n",
    "from keras.layers import ZeroPadding2D\n",
    "# from keras.preprocessing import image\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "\n",
    "# from PIL import Image\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.image as mpimg\n",
    "\n",
    "from imutils import paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set directory path\n",
    "root_dir = os.path.abspath('C:\\\\Users\\\\LYC\\\\Desktop\\\\SFU\\\\Fall 2020 Courses\\\\STAT 440\\\\Module 3') \n",
    "data_dir = os.path.join(root_dir, 'tr') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = os.path.join(data_dir, 'img') # training img file directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadImages(path):\n",
    "    '''Put files into lists and return them as one list with all images \n",
    "     in the folder'''\n",
    "    image_files = sorted([os.path.join(path, 'img', file)\n",
    "                          for file in os.listdir(path + \"/img\")\n",
    "                          if file.endswith('.jpg')])\n",
    "    return image_files\n",
    "\n",
    "img = loadImages(data_dir) # all the directories for training images\n",
    "\n",
    "def load_test_Images(path):\n",
    "    '''Put files into lists and return them as one list with all images \n",
    "     in the folder'''\n",
    "    image_files = sorted([os.path.join(path, 'te', file)\n",
    "                          for file in os.listdir(path + \"/te\")\n",
    "                          if file.endswith('.jpg')])\n",
    "    return image_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadLbl(path):\n",
    "    '''Put files into lists and return them as one list with all images \n",
    "     in the folder'''\n",
    "    lbl_files = sorted([os.path.join(path, 'lbl', file)\n",
    "                          for file in os.listdir(path + \"/lbl\")\n",
    "                          if file.endswith('.txt')])\n",
    "    return lbl_files\n",
    "\n",
    "lbl = loadLbl(data_dir) # all the labels for training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create csv file, name column = image file names, label column = corresponding label\n",
    "\n",
    "labels = []\n",
    "for i in range(len(lbl)):\n",
    "    img2 = img[i][-11:]\n",
    "    label = open(lbl[i], \"r\").read()\n",
    "    label = label.replace(\"\\n\", \"\")\n",
    "    labels.append((img2, label))\n",
    "\n",
    "labels = pd.DataFrame(labels, columns=['name', 'label'])\n",
    "labels.to_csv('labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(os.path.join(root_dir, 'labels.csv'), dtype = 'str') # load the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ImageDataGenerator\n",
    "# this will pull image from source folder, \n",
    "# then perform random transformations we initialized, \n",
    "# then feed the transformed image to the next step\n",
    "\n",
    "aug = ImageDataGenerator(\n",
    "    validation_split = 0.2, # creates validation set, no need to do train_test_split\n",
    "    rescale=1./255.,\n",
    "\trotation_range=30,\n",
    "\tzoom_range=0.15,\n",
    "\twidth_shift_range=0.2,\n",
    "\theight_shift_range=0.2,\n",
    "\tshear_range=0.15,\n",
    "\thorizontal_flip=True,\n",
    "\tfill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1011 validated image filenames belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "# create pipeline for feeding training data from source folder to training model\n",
    "\n",
    "train_generator=aug.flow_from_dataframe(dataframe=train_data,\n",
    "                                            directory=(train), #directory for training images, IMPORTANT\n",
    "                                            x_col='name',\n",
    "                                            y_col='label',\n",
    "                                            subset='training', # identify as training data\n",
    "                                            batch_size=32, \n",
    "                                            seed=42,\n",
    "                                            shuffle=True,\n",
    "                                            class_mode='categorical',\n",
    "                                            target_size=(56,56)) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 252 validated image filenames belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "# create pipeline for feeding validation data from source folder to training model\n",
    "# the validation data will be split from training data automatically\n",
    "\n",
    "valid_generator=aug.flow_from_dataframe(dataframe=train_data,\n",
    "                                            directory=(train), #directory for training images, IMPORTANT\n",
    "                                            x_col='name',\n",
    "                                            y_col='label',\n",
    "                                            subset='validation', #identify as validation data\n",
    "                                            batch_size=32, \n",
    "                                            seed=42,\n",
    "                                            class_mode='categorical',\n",
    "                                            target_size=(56,56))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model architecture\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(2, 2), strides=2, activation='relu',input_shape=(56,56,3)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(32, kernel_size=(2, 2), strides=2, activation='relu'))\n",
    "model.add(Conv2D(64, (2, 2), activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=tf.optimizers.Adam(lr=0.001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "31/31 [==============================] - 5s 170ms/step - loss: 0.2434 - accuracy: 0.8989 - val_loss: 1.0438 - val_accuracy: 0.7232\n",
      "Epoch 2/25\n",
      "31/31 [==============================] - 5s 155ms/step - loss: 0.2575 - accuracy: 0.9040 - val_loss: 1.1068 - val_accuracy: 0.7679\n",
      "Epoch 3/25\n",
      "31/31 [==============================] - 5s 152ms/step - loss: 0.2066 - accuracy: 0.9244 - val_loss: 1.1227 - val_accuracy: 0.7902\n",
      "Epoch 4/25\n",
      "31/31 [==============================] - 5s 157ms/step - loss: 0.2488 - accuracy: 0.9132 - val_loss: 1.0436 - val_accuracy: 0.7634\n",
      "Epoch 5/25\n",
      "31/31 [==============================] - 5s 155ms/step - loss: 0.2200 - accuracy: 0.9224 - val_loss: 1.0228 - val_accuracy: 0.7723\n",
      "Epoch 6/25\n",
      "31/31 [==============================] - 5s 153ms/step - loss: 0.2368 - accuracy: 0.9234 - val_loss: 1.1126 - val_accuracy: 0.7411\n",
      "Epoch 7/25\n",
      "31/31 [==============================] - 5s 157ms/step - loss: 0.2497 - accuracy: 0.9183 - val_loss: 1.3537 - val_accuracy: 0.7679\n",
      "Epoch 8/25\n",
      "31/31 [==============================] - 5s 158ms/step - loss: 0.1993 - accuracy: 0.9285 - val_loss: 1.2414 - val_accuracy: 0.7321\n",
      "Epoch 9/25\n",
      "31/31 [==============================] - 5s 162ms/step - loss: 0.2601 - accuracy: 0.9070 - val_loss: 0.9845 - val_accuracy: 0.7857\n",
      "Epoch 10/25\n",
      "31/31 [==============================] - 6s 179ms/step - loss: 0.2477 - accuracy: 0.9132 - val_loss: 1.3249 - val_accuracy: 0.7455\n",
      "Epoch 11/25\n",
      "31/31 [==============================] - 5s 169ms/step - loss: 0.2373 - accuracy: 0.9142 - val_loss: 1.0952 - val_accuracy: 0.7723\n",
      "Epoch 12/25\n",
      "31/31 [==============================] - 5s 167ms/step - loss: 0.2814 - accuracy: 0.8897 - val_loss: 1.2882 - val_accuracy: 0.7500\n",
      "Epoch 13/25\n",
      "31/31 [==============================] - 5s 169ms/step - loss: 0.2806 - accuracy: 0.8927 - val_loss: 1.1956 - val_accuracy: 0.7589\n",
      "Epoch 14/25\n",
      "31/31 [==============================] - 5s 174ms/step - loss: 0.2959 - accuracy: 0.8897 - val_loss: 1.0984 - val_accuracy: 0.7500\n",
      "Epoch 15/25\n",
      "31/31 [==============================] - 5s 173ms/step - loss: 0.2178 - accuracy: 0.9213 - val_loss: 1.1707 - val_accuracy: 0.7455\n",
      "Epoch 16/25\n",
      "31/31 [==============================] - 6s 201ms/step - loss: 0.2020 - accuracy: 0.9234 - val_loss: 1.2639 - val_accuracy: 0.7188\n",
      "Epoch 17/25\n",
      "31/31 [==============================] - 5s 176ms/step - loss: 0.2553 - accuracy: 0.9183 - val_loss: 1.2706 - val_accuracy: 0.7277\n",
      "Epoch 18/25\n",
      "31/31 [==============================] - 5s 167ms/step - loss: 0.2099 - accuracy: 0.9305 - val_loss: 1.0240 - val_accuracy: 0.7991\n",
      "Epoch 19/25\n",
      "31/31 [==============================] - 6s 178ms/step - loss: 0.2095 - accuracy: 0.9162 - val_loss: 1.3352 - val_accuracy: 0.7857\n",
      "Epoch 20/25\n",
      "31/31 [==============================] - 5s 165ms/step - loss: 0.2277 - accuracy: 0.9183 - val_loss: 1.2658 - val_accuracy: 0.7545\n",
      "Epoch 21/25\n",
      "31/31 [==============================] - 5s 164ms/step - loss: 0.2455 - accuracy: 0.9152 - val_loss: 1.3002 - val_accuracy: 0.7679\n",
      "Epoch 22/25\n",
      "31/31 [==============================] - 6s 179ms/step - loss: 0.2537 - accuracy: 0.9101 - val_loss: 1.0049 - val_accuracy: 0.7991\n",
      "Epoch 23/25\n",
      "31/31 [==============================] - 5s 164ms/step - loss: 0.2173 - accuracy: 0.9183 - val_loss: 1.3082 - val_accuracy: 0.7232\n",
      "Epoch 24/25\n",
      "31/31 [==============================] - 5s 165ms/step - loss: 0.3985 - accuracy: 0.8744 - val_loss: 1.0959 - val_accuracy: 0.7411\n",
      "Epoch 25/25\n",
      "31/31 [==============================] - 5s 175ms/step - loss: 0.2875 - accuracy: 0.9019 - val_loss: 0.8737 - val_accuracy: 0.8170\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x333029d0>"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
    "STEP_SIZE_VALID=valid_generator.n//valid_generator.batch_size\n",
    "\n",
    "model.fit(train_generator,\n",
    "          steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "          validation_data=valid_generator,\n",
    "          validation_steps=STEP_SIZE_VALID,\n",
    "          epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare test data\n",
    "#load test set\n",
    "test = load_test_Images(root_dir)\n",
    "#read and store test image\n",
    "test_image = []\n",
    "for i in test:\n",
    "    pic = load_img(i, target_size=(56,56,1), grayscale=False)\n",
    "    a = img_to_array(pic)\n",
    "    a = a/255.\n",
    "    a = a.astype('float32')\n",
    "    test_image.append(a)\n",
    "    \n",
    "test = np.stack(test_image)\n",
    "\n",
    "prediction = model.predict_classes(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = prediction + 1 # for some reason prediction predicts from 0 - 5 instead of 1 - 6, so we add 1 to each prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datafame with prediction and test picture IDs\n",
    "from pandas import *\n",
    "idx = pd.Series(Int64Index(range(1264,2528)))\n",
    "prediction = pd.Series(prediction)\n",
    "\n",
    "data = concat([idx,prediction],axis = 1)\n",
    "data.columns = ['Id',\"Prediction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating submission file\n",
    "data.to_csv('kaggle_submission.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\LYC\\Desktop\\SFU\\Fall 2020 Courses\\STAT 440\\Module 3\\assets\n"
     ]
    }
   ],
   "source": [
    "# save the model to directory\n",
    "model.save(root_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
